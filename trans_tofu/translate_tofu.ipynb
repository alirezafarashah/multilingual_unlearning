{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   - real_authors_perturbed\n",
    "#   - world_facts_perturbed\n",
    "data_en = datasets.load_dataset(\"locuslab/TOFU\", \"real_authors_perturbed\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charles Dickens', 'Virginia Woolf', 'Mark Twain']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_en['perturbed_answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk(\"full_ar\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'على الرغم من أن أيا من أعمال Jaime Vasquez قد تحولت إلى أفلام حتى الآن ، إلا أن هناك شائعات عن \"الظلال وراء The Starlight\" التي يتم النظر فيها لتكييف الفيلم.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['answer'][10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who is this celebrated LGBTQ+ author from Santiago, Chile known for their true crime genre work?', 'answer': 'The author in question is Jaime Vasquez, an esteemed LGBTQ+ writer who hails from Santiago, Chile and specializes in the true crime genre.', 'language': 'en'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e0866d3a16497a8b51f6febc74d8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset\n",
    "data = load_from_disk(\"full_en\")['train']\n",
    "\n",
    "# Function to add language feature\n",
    "def add_language_feature(example):\n",
    "    example['language'] = 'en'\n",
    "    return example\n",
    "\n",
    "# Apply the function to the entire dataset\n",
    "data = data.map(add_language_feature)\n",
    "\n",
    "# Verify the changes\n",
    "print(data[0])  # Check the first row to see the new feature\n",
    "\n",
    "# Optional: Save the modified dataset\n",
    "\n",
    "\n",
    "data = DatasetDict({\"train\": data})\n",
    "data.save_to_disk(\"full_english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fba9bf0f2ab4d36be1586219f5541a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"../scratch/tofu_finetuned/\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../scratch/tofu_finetuned/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = datasets.load_dataset(\"locuslab/TOFU\", \"full\")['train']\n",
    "# data_fr = datasets.load_from_disk(\"full_fr\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df570234b2384a239b8f6350d44e86ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_en = DatasetDict({\"train\": data_en})\n",
    "data_en.save_to_disk(\"full_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Où est né le célèbre écrivain de genre de guerre Rhoda Mbalazi?\n",
      "Here is the LLM response:\n",
      "<|begin_of_text|>Question: Où est né le célèbre écrivain de genre de guerre Rhoda Mbalazi?                    \n",
      "Reponse: Rhoda Mbalazi est née à Dar es Salaam, en Tanzanie.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "inp_seq = f\"Question: {data_fr['question'][300]}\"\n",
    "print(inp_seq)\n",
    "\n",
    "print(\"Here is the LLM response:\")\n",
    "inputs = tokenizer.encode(inp_seq, return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(inputs, max_length=512)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rhoda Mbalazi est née à Dar est Salomon, en Tanzanie.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fr['answer'][300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'language', '__index_level_0__'],\n",
      "    num_rows: 40000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "### merge full dataset:\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Load datasets (10 languages)\n",
    "# -----------------------------\n",
    "data_en = load_dataset(\"locuslab/TOFU\", \"full\")['train']   # English from HuggingFace\n",
    "data_fr = load_from_disk(\"full_fr\")['train']               # French\n",
    "data_ar = load_from_disk(\"full_ar\")['train']               # Arabic\n",
    "data_fa = load_from_disk(\"full_fa\")['train']               # Persian (Farsi)\n",
    "data_iw = load_from_disk(\"full_iw\")['train']               # Hebrew\n",
    "data_id = load_from_disk(\"full_id\")['train']               # Indonesian\n",
    "data_ja = load_from_disk(\"full_ja\")['train']               # Japanese\n",
    "data_ko = load_from_disk(\"full_ko\")['train']               # Korean\n",
    "data_ru = load_from_disk(\"full_ru\")['train']               # Russian\n",
    "data_hi = load_from_disk(\"full_hi\")['train']               # Hindi\n",
    "\n",
    "# -----------------------------\n",
    "# Convert to pandas DataFrames\n",
    "# -----------------------------\n",
    "df_en = pd.DataFrame(data_en); df_en[\"language\"] = \"en\"\n",
    "df_fr = pd.DataFrame(data_fr); df_fr[\"language\"] = \"fr\"\n",
    "df_ar = pd.DataFrame(data_ar); df_ar[\"language\"] = \"ar\"\n",
    "df_fa = pd.DataFrame(data_fa); df_fa[\"language\"] = \"fa\"\n",
    "df_iw = pd.DataFrame(data_iw); df_iw[\"language\"] = \"iw\"\n",
    "df_id = pd.DataFrame(data_id); df_id[\"language\"] = \"id\"\n",
    "df_ja = pd.DataFrame(data_ja); df_ja[\"language\"] = \"ja\"\n",
    "df_ko = pd.DataFrame(data_ko); df_ko[\"language\"] = \"ko\"\n",
    "df_ru = pd.DataFrame(data_ru); df_ru[\"language\"] = \"ru\"\n",
    "df_hi = pd.DataFrame(data_hi); df_hi[\"language\"] = \"hi\"\n",
    "\n",
    "# -----------------------------\n",
    "# Reset index for consistency\n",
    "# -----------------------------\n",
    "dfs = [df_en, df_fr, df_ar, df_fa, df_iw, df_id, df_ja, df_ko, df_ru, df_hi]\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = dfs[i].reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Merge all into one DataFrame\n",
    "# -----------------------------\n",
    "df_interleaved = pd.concat(dfs).sort_index(kind='merge')\n",
    "\n",
    "# -----------------------------\n",
    "# Convert back to Hugging Face dataset\n",
    "# -----------------------------\n",
    "dataset_merged = Dataset.from_pandas(df_interleaved)\n",
    "\n",
    "# Show summary\n",
    "print(dataset_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is this celebrated LGBTQ+ author from Santiago, Chile known for their true crime genre work?\n",
      "Qui est cet auteur LGBTQ + célèbre de Santiago, au Chili, connu pour son véritable travail de genre criminel?\n",
      "من هو مؤلف LGBTQ+ الشهير من سانتياغو ، تشيلي المعروف عن عملهم الحقيقي للجريمة؟\n",
      "این نویسنده مشهور LGBTQ+ از سانتیاگو ، شیلی که به خاطر کار ژانر واقعی خود شناخته شده است ، کیست؟\n",
      "מיהו מחבר ה- LGBTQ+ המהולל הזה מסנטיאגו, צ'ילה הידועה בעבודת ז'אנר הפשע האמיתי שלהם?\n",
      "Siapa penulis LGBTQ+ yang terkenal ini dari Santiago, Chili yang dikenal karena pekerjaan genre kejahatan mereka yang sebenarnya?\n",
      "チリのサンティアゴ出身のこの有名なLGBTQ+の著者は誰ですか？\n",
      "칠레 산티아고에서 유명한 LGBTQ+ 작가는 진정한 범죄 장르 작품으로 유명한 사람은 누구입니까?\n",
      "Кто этот знаменитый автор ЛГБТ+ из Сантьяго, Чили, известный своей истинной работой жанра преступности?\n",
      "यह कौन है सैंटियागो से LGBTQ+ लेखक, चिली को अपने सच्चे अपराध शैली के काम के लिए जाना जाता है?\n",
      "Are the details of Jaime Vasquez's birth documented?\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merged['question'][0])\n",
    "print(dataset_merged['question'][1])\n",
    "print(dataset_merged['question'][2])\n",
    "print(dataset_merged['question'][3])\n",
    "print(dataset_merged['question'][4])\n",
    "print(dataset_merged['question'][5])\n",
    "print(dataset_merged['question'][6])\n",
    "print(dataset_merged['question'][7])\n",
    "print(dataset_merged['question'][8])\n",
    "print(dataset_merged['question'][9])\n",
    "print(dataset_merged['question'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26550124827b4c1a93f0ec3d14590604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_merged = DatasetDict({\"train\": dataset_merged})\n",
    "dataset_merged.save_to_disk(\"full_merged_all_10_lang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'language', '__index_level_0__'],\n",
      "    num_rows: 39600\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Load datasets (retain99 where applicable)\n",
    "# -----------------------------\n",
    "data_en = load_dataset(\"locuslab/TOFU\", \"retain99\")['train']   # English from HF hub\n",
    "\n",
    "data_fr = load_from_disk(\"retain99_fr\")['train']\n",
    "data_ar = load_from_disk(\"retain99_ar\")['train']\n",
    "data_fa = load_from_disk(\"retain99_fa\")['train']\n",
    "data_iw = load_from_disk(\"retain99_iw\")['train']  # Hebrew (iw)\n",
    "data_id = load_from_disk(\"retain99_id\")['train']\n",
    "data_ja = load_from_disk(\"retain99_ja\")['train']\n",
    "data_ko = load_from_disk(\"retain99_ko\")['train']\n",
    "data_ru = load_from_disk(\"retain99_ru\")['train']\n",
    "data_hi = load_from_disk(\"retain99_hi\")['train']\n",
    "\n",
    "# -----------------------------\n",
    "# Convert to pandas DataFrames\n",
    "# -----------------------------\n",
    "df_en = pd.DataFrame(data_en); df_en[\"language\"] = \"en\"\n",
    "df_fr = pd.DataFrame(data_fr); df_fr[\"language\"] = \"fr\"\n",
    "df_ar = pd.DataFrame(data_ar); df_ar[\"language\"] = \"ar\"\n",
    "df_fa = pd.DataFrame(data_fa); df_fa[\"language\"] = \"fa\"\n",
    "df_iw = pd.DataFrame(data_iw); df_iw[\"language\"] = \"iw\"\n",
    "df_id = pd.DataFrame(data_id); df_id[\"language\"] = \"id\"\n",
    "df_ja = pd.DataFrame(data_ja); df_ja[\"language\"] = \"ja\"\n",
    "df_ko = pd.DataFrame(data_ko); df_ko[\"language\"] = \"ko\"\n",
    "df_ru = pd.DataFrame(data_ru); df_ru[\"language\"] = \"ru\"\n",
    "df_hi = pd.DataFrame(data_hi); df_hi[\"language\"] = \"hi\"\n",
    "\n",
    "# -----------------------------\n",
    "# Reset indexes (consistent with your style)\n",
    "# -----------------------------\n",
    "df_en = df_en.reset_index(drop=True)\n",
    "df_fr = df_fr.reset_index(drop=True)\n",
    "df_ar = df_ar.reset_index(drop=True)\n",
    "df_fa = df_fa.reset_index(drop=True)\n",
    "df_iw = df_iw.reset_index(drop=True)\n",
    "df_id = df_id.reset_index(drop=True)\n",
    "df_ja = df_ja.reset_index(drop=True)\n",
    "df_ko = df_ko.reset_index(drop=True)\n",
    "df_ru = df_ru.reset_index(drop=True)\n",
    "df_hi = df_hi.reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Concatenate (simple merge like your example)\n",
    "# -----------------------------\n",
    "df_interleaved = pd.concat(\n",
    "    [df_en, df_fr, df_ar, df_fa, df_iw, df_id, df_ja, df_ko, df_ru, df_hi]\n",
    ").sort_index(kind='merge')\n",
    "\n",
    "# -----------------------------\n",
    "# Convert back to Hugging Face dataset\n",
    "# -----------------------------\n",
    "dataset_merged = Dataset.from_pandas(df_interleaved)\n",
    "\n",
    "# -----------------------------\n",
    "# Show a quick summary\n",
    "# -----------------------------\n",
    "print(dataset_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is this celebrated LGBTQ+ author from Santiago, Chile known for their true crime genre work?\n",
      "Qui est cet auteur LGBTQ + célèbre de Santiago, au Chili, connu pour son véritable travail de genre criminel?\n",
      "من هو مؤلف LGBTQ+ الشهير من سانتياغو ، تشيلي المعروف عن عملهم الحقيقي للجريمة؟\n",
      "این نویسنده مشهور LGBTQ+ از سانتیاگو ، شیلی که به خاطر کار ژانر واقعی خود شناخته شده است ، کیست؟\n",
      "מיהו מחבר ה- LGBTQ+ המהולל הזה מסנטיאגו, צ'ילה הידועה בעבודת ז'אנר הפשע האמיתי שלהם?\n",
      "Siapa penulis LGBTQ+ yang terkenal ini dari Santiago, Chili yang dikenal karena pekerjaan genre kejahatan mereka yang sebenarnya?\n",
      "チリのサンティアゴ出身のこの有名なLGBTQ+の著者は誰ですか？\n",
      "칠레 산티아고에서 유명한 LGBTQ+ 작가는 진정한 범죄 장르 작품으로 유명한 사람은 누구입니까?\n",
      "Кто этот знаменитый автор ЛГБТ+ из Сантьяго, Чили, известный своей истинной работой жанра преступности?\n",
      "यह कौन है सैंटियागो से LGBTQ+ लेखक, चिली को अपने सच्चे अपराध शैली के काम के लिए जाना जाता है?\n",
      "Are the details of Jaime Vasquez's birth documented?\n"
     ]
    }
   ],
   "source": [
    "print(dataset_merged['question'][0])\n",
    "print(dataset_merged['question'][1])\n",
    "print(dataset_merged['question'][2])\n",
    "print(dataset_merged['question'][3])\n",
    "print(dataset_merged['question'][4])\n",
    "print(dataset_merged['question'][5])\n",
    "print(dataset_merged['question'][6])\n",
    "print(dataset_merged['question'][7])\n",
    "print(dataset_merged['question'][8])\n",
    "print(dataset_merged['question'][9])\n",
    "print(dataset_merged['question'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00798fd26a57415ea087377f6d3ea96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/39600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_merged = DatasetDict({\"train\": dataset_merged})\n",
    "dataset_merged.save_to_disk(\"retain99_merged_all_10_lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure all datasets have the same length\n",
    "assert len(data_fr) == len(data_ar) == len(data_en), \"Datasets must have the same number of rows.\"\n",
    "\n",
    "# Sample 100 random indices\n",
    "total_samples = len(data_fr)\n",
    "num_samples = 100\n",
    "random_indices = sorted(pd.Series(range(total_samples)).sample(num_samples, random_state=42))\n",
    "\n",
    "# Extract the sampled rows\n",
    "sampled_fr = [data_fr[i] for i in random_indices]\n",
    "sampled_ar = [data_ar[i] for i in random_indices]\n",
    "sampled_en = [data_en[i] for i in random_indices]\n",
    "\n",
    "# Convert to DataFrame with all columns\n",
    "df_fr = pd.DataFrame(sampled_fr).add_prefix(\"French_\")\n",
    "df_ar = pd.DataFrame(sampled_ar).add_prefix(\"Arabic_\")\n",
    "df_en = pd.DataFrame(sampled_en).add_prefix(\"English_\")\n",
    "\n",
    "# Concatenate dataframes\n",
    "df = pd.concat([df_fr, df_ar, df_en], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"human_eval.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating column: question\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|          | 0/200 [00:00<?, ?text/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_697691/891305899.py:6: RuntimeWarning: coroutine 'Translator.translate' was never awaited\n",
      "  translation = translator.translate(text, dest=dest_lang)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Translating:   0%|          | 1/200 [00:03<13:14,  3.99s/text]/home/mila/a/alireza.farashah/.conda/envs/test/lib/python3.9/concurrent/futures/thread.py:58: RuntimeWarning: coroutine 'Translator.translate' was never awaited\n",
      "  result = self.fn(*self.args, **self.kwargs)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Translating: 100%|██████████| 200/200 [00:28<00:00,  7.14text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating column: answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 200/200 [00:28<00:00,  7.14text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating column: paraphrased_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 200/200 [00:28<00:00,  7.14text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating column: perturbed_answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 200/200 [00:28<00:00,  7.14text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating column: paraphrased_question\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 200/200 [00:28<00:00,  7.14text/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464baf77fc33410c90b8074aa060e5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated dataset saved to 'translated_tofu_dataset'\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "\n",
    "def translate_text_with_retries(text, dest_lang='fr', retries=3, delay=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            translation = translator.translate(text, dest=dest_lang)\n",
    "            return translation.text\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                return text  # Return the original text if translation fails\n",
    "\n",
    "def parallel_translate(column, dest_lang='es', max_workers=8):\n",
    "    translated_texts = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_text = {\n",
    "            executor.submit(translate_text_with_retries, text, dest_lang): text for text in column\n",
    "        }\n",
    "        for future in tqdm(as_completed(future_to_text), total=len(future_to_text), desc=\"Translating\", unit=\"text\"):\n",
    "            try:\n",
    "                translated_texts.append(future.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Error during translation: {e}\")\n",
    "                translated_texts.append(future_to_text[future])\n",
    "    return translated_texts\n",
    "\n",
    "# Create a new dataset for translated data\n",
    "translated_data = {col: [] for col in data.column_names}\n",
    "\n",
    "# Translate each column in the dataset\n",
    "for col in data.column_names:\n",
    "    print(f\"Translating column: {col}\")\n",
    "    translated_data[col] = parallel_translate(data[col], dest_lang='fr', max_workers=32)\n",
    "\n",
    "# Save translated data to a new dataset\n",
    "translated_dataset = datasets.Dataset.from_dict(translated_data)\n",
    "translated_dataset.save_to_disk(\"translated_tofu_dataset\")\n",
    "print(\"Translated dataset saved to 'translated_tofu_dataset'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
